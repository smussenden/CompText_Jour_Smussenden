---
title: "Lynching Text Analysis"
author: "Rob Wells"
date: '2022-09-27'
output: html_document
---

# Jour 389/689 Fall 2024:

Use the following code as a template with a new dataset:

black_press_extracted_text_june_22_2024.csv

See the blue hashtag instructions in the code chunks, such as:

#import df

#Show range of years covered

#Create chart of years

And do the same with the new dataset: black_press_extracted_text_june_22_2024.csv

Hand in your completed assignment by uploading to your personal GitHub repository, obtaining the URL and submitting that to Elms

```{r}
#load tidyverse, tidytext, rio and quanteda libraries

```

```{r}
#Import dataframe 


lynch <- read_csv("../data/articles_oct_19.csv")

```

# plot of years covered

```{r}

#Show range of years covered
years_ct <- lynch %>%
  distinct(filename, .keep_all = TRUE) %>% 
  count(year)

y <- lynch %>%
  distinct(filename, .keep_all = TRUE)

#Create chart of years
ggplot(years_ct,aes(x = year, y = n,
             fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "Years of Lynching Coverage",
       subtitle = "Based in 7,162 extracted articles",
       caption = "Graphic by Rob Wells, 10-30-2023",
       y="Articles",
       x="Year")

# ggsave("../output_images_tables/Figure2_years_lynching_coverage_10.30.23.png",device = "png",width=9,height=6, dpi=800)
```

# By decade

## post1940

```{r}
#Filter articles from 1940s forward
post1940 <-  lynch %>% 
  filter(year >= 1940)

post1940 %>% 
  select(filename) %>% 
 distinct(filename, .keep_all = TRUE) %>% 
  count(filename) %>% 
  summarize(total =sum(n)) 
#62 articles 

statespost1940 <- post1940 %>% 
  select(newspaper_state, filename) %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))

statespost1940 %>% 
  select(newspaper_state, n) %>% 
slice_max(n, n=10)
#newspaper_state

# Michigan	20			
# Minnesota	18			
# District of Columbia	5			
# Nebraska	4			
# Illinois	3			
# Mississippi	2			
# North Carolina	2			
# Washington	2			
# Alaska	1			
# Arizona	1	

#Fact Check
#sum(statesthe1850s$n)

x <- post1940 %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  arrange(date)

#write_csv(x, "post1940_index.csv")
```

# Tokenize

```{r}

stories <- str_replace_all(post1940$sentence, "- ", "")
stories_df <- tibble(stories,)

# unnest includes lower, punct removal

stories_tokenized <- stories_df %>%
  unnest_tokens(word,stories)

stories_tokenized

#Remove stopwords

data(stop_words)

stories_tokenized <- stories_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

# Word Count

story_word_ct <- stories_tokenized %>%
  count(word, sort=TRUE)

#write_csv(lynch_word_ct, "lynching_corpus_word_count.csv")

```

# Bigrams

```{r}
stories_bigrams <- stories_df %>%
  unnest_tokens(bigram, stories, token="ngrams", n=2)

stories_bigrams

#Filter out stop words.


stories_bigrams_separated <- stories_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

stories_bigrams_filtered <- stories_bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

stories_bigram_cts <- stories_bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

# put back into bigram form if we want to use them
stories_bigrams_united <- stories_bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#replace Date for the decade analyzed
stories_bigram_cts_post1940 <- stories_bigram_cts %>% 
  mutate(decade = "post1940")

#write_csv(stories_bigram_cts_post1940, "../output/post1940_lynch_bigram_count.csv")

```

# Trigrams

```{r}
stories_trigrams <- stories_df %>%
  unnest_tokens(trigram, stories, token="ngrams", n=3)

stories_trigrams_separated <- stories_trigrams %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

stories_trigrams_ct <- stories_trigrams_separated %>%
  count(word1, word2, word3, sort = TRUE)

#filtered
# stories_trigrams_filtered <- stories_trigrams_separated %>%
#   filter(!word1 %in% stop_words$word) %>%
#   filter(!word2 %in% stop_words$word) %>%
#   filter(!word3 %in% stop_words$word)
# 
# stories_trigrams_ct <- stories_trigrams_filtered %>%
#   count(word1, word2, word3, sort = TRUE)

#replace Date for the decade analyzed
stories_trigrams_ct_post1940 <- stories_trigrams_ct %>% 
  mutate(decade = "post1940")

write_csv(stories_trigrams_ct_post1940, "../output/post1940_lynch_trigram_count.csv")


```

#Compile DFs

```{r}
stories_bigram_cts_pre1850s <- read.csv("../output/pre1850s_lynch_bigram_count.csv")
stories_bigram_cts_the1850s <- read.csv("../output/the1850s_lynch_bigram_count.csv")
stories_bigram_cts_the1860s <- read.csv("../output/1860s_lynch_bigram_count.csv")
stories_bigram_cts_the1870s <- read.csv("../output/1870s_lynch_bigram_count.csv")
stories_bigram_cts_the1880s <- read.csv("../output/1880s_lynch_bigram_count.csv")
stories_bigram_cts_the1890s <- read.csv("../output/1890s_lynch_bigram_count.csv")
stories_bigram_cts_the1900s <- read.csv("../output/1900s_lynch_bigram_count.csv")
stories_bigram_cts_the1910  <- read.csv("../output/1910s_lynch_bigram_count.csv")
stories_bigram_cts_the1920s <- read.csv("../output/1920s_lynch_bigram_count.csv")
```

```{r}
#Compile DFs

bigrams_all <- rbind(stories_bigram_cts_pre1850s,stories_bigram_cts_the1850s, stories_bigram_cts_the1860s, stories_bigram_cts_the1870s, stories_bigram_cts_the1880s, stories_bigram_cts_the1890s, stories_bigram_cts_the1900s, stories_bigram_cts_the1910,stories_bigram_cts_the1920s, stories_bigram_cts_1930s, stories_bigram_cts_post1940) 


write.csv(bigrams_all, "../output/all_bigrams_11.10.csv")
```

```{r}
stories_QUINTgrams <- stories_df %>%
  unnest_tokens(phrase, stories, token="ngrams", n=5)

stories_QUINTgrams_ct <- stories_QUINTgrams %>%
  count(phrase, sort=TRUE)

#write_csv(stories_QUINTgrams_ct, "stories_corpus_quintgram_count.csv")

stories_QUINTgrams_ct

```

```{r}
# plotting for fun and profit
#NEEDS TO BE FIXED
story_word_ct %>%
  filter(n >= 5000) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(n, word)) + 
  geom_col() +
  labs(y = NULL) + 
  ggtitle("Words mentioned 5000 times or more in identified lynching articles")

```

*If we want to look at trigrams for narrative, maybe we don't drop the stop words? Might be more likely to catch turns of phrase.*

# Quanteda

```{r}
#install.packages("readtext")
library(quanteda)
library(readtext)
lynch1 <- readtext("../narratives/article_text_test")
```

### creates index with metadata

```{r}
###
# List out text files that match pattern .txt, create DF
###

files <- list.files("../article_text_7_15/article_text/", pattern="*.txt") %>% 
  as.data.frame() %>%
  rename(filename = 1) %>%
  filter(!str_detect(filename,"log"))


###
# Load 638 stories provided by jack, create join column, join to files list
###

jackindex <- read_csv("../article_text_7_15/article_text/LayoutBoxes_index.csv") %>%
  mutate(filename = paste0(file_id,"_",article_id,".txt")) %>%
  inner_join(files) %>%
  mutate(filepath = paste0("../article_text_7_15/article_text/",filename))
```

### adds metadata to corpus

```{r}
lynch1 <- lynch1 %>% 
  inner_join(jackindex, by=c("doc_id"="filename"))


#Other options
#summary(corpus_subset(data_corpus_inaugural, President == "Adams"))

```

```{r}

my_corpus <- corpus(lynch1)  # build a new corpus from the texts
summary(my_corpus)


```

### subset corpus

```{r}

x1920s <- summary(corpus_subset(my_corpus, year > 1920))

```

## kwic

```{r}

quanteda_test <- kwic(my_corpus, "lynch", valuetype = "regex") %>% as.data.frame()


quanteda_test <- kwic(my_corpus, "torture", valuetype = "regex") %>% as.data.frame()

quanteda_test <- kwic(my_corpus, "watts", valuetype = "regex") %>% as.data.frame()

  
#write.csv(quanteda_test, "quanteda_test.csv")

```

### 
